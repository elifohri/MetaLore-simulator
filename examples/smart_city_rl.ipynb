{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating `mobile-env:smart-city`\n",
    "\n",
    "`mobile-env` is a simple and open environment for training, testing, and evaluating a decentralized metaverse environment.\n",
    "\n",
    "* `mobile-env:smart-city` is written in pure Python\n",
    "* It allows simulating various scenarios with moving users in a cellular network with a single base station and multiple stationary sensors\n",
    "* `mobile-env:smart-city` implements the standard [Gymnasium](https://gymnasium.farama.org/) (previously [OpenAI Gym](https://gym.openai.com/)) interface such that it can be used with all common frameworks for reinforcement learning\n",
    "* `mobile-env:smart-city` is not restricted to reinforcement learning approaches but can also be used with conventional control approaches or dummy benchmark algorithms\n",
    "* It can be configured easily (e.g., adjusting number and movement of users, properties of cells, etc.)\n",
    "* It is also easy to extend `mobile-env:smart-city`, e.g., implementing different observations, actions, or reward\n",
    "\n",
    "As such `mobile-env:smart-city` is a simple platform to ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration Steps:**\n",
    "\n",
    "This demonstration consists of the following steps:\n",
    "\n",
    "1. Installation and usage of `mobile-env` with dummy actions\n",
    "2. Configuration of `mobile-env` and adjustment of the observation space (optional)\n",
    "3. Training a single-agent reinforcement learning approach with [`stable-baselines3`](https://github.com/DLR-RM/stable-baselines3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stable-baselines3==2.0.0 in /home/elifohri/.local/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: tensorboard in /home/elifohri/.local/lib/python3.10/site-packages (2.17.1)\n",
      "Requirement already satisfied: torch>=1.11 in /home/elifohri/.local/lib/python3.10/site-packages (from stable-baselines3==2.0.0) (2.4.1)\n",
      "Requirement already satisfied: gymnasium==0.28.1 in /home/elifohri/.local/lib/python3.10/site-packages (from stable-baselines3==2.0.0) (0.28.1)\n",
      "Requirement already satisfied: cloudpickle in /home/elifohri/.local/lib/python3.10/site-packages (from stable-baselines3==2.0.0) (3.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/elifohri/.local/lib/python3.10/site-packages (from stable-baselines3==2.0.0) (3.9.2)\n",
      "Requirement already satisfied: pandas in /home/elifohri/.local/lib/python3.10/site-packages (from stable-baselines3==2.0.0) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/elifohri/.local/lib/python3.10/site-packages (from stable-baselines3==2.0.0) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/elifohri/.local/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (4.11.0)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /home/elifohri/.local/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (1.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/elifohri/.local/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (0.0.4)\n",
      "Requirement already satisfied: six>1.9 in /home/elifohri/.local/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/elifohri/.local/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/elifohri/.local/lib/python3.10/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/elifohri/.local/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/elifohri/.local/lib/python3.10/site-packages (from tensorboard) (5.28.1)\n",
      "Requirement already satisfied: packaging in /home/elifohri/.local/lib/python3.10/site-packages (from tensorboard) (24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard) (59.6.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/elifohri/.local/lib/python3.10/site-packages (from tensorboard) (1.66.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/elifohri/.local/lib/python3.10/site-packages (from tensorboard) (3.0.4)\n",
      "Requirement already satisfied: sympy in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (1.13.2)\n",
      "Requirement already satisfied: filelock in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.16.0)\n",
      "Requirement already satisfied: networkx in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (2.20.5)\n",
      "Requirement already satisfied: fsspec in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (9.1.0.70)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/elifohri/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/elifohri/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->stable-baselines3==2.0.0) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/elifohri/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/elifohri/.local/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0) (1.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/elifohri/.local/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/elifohri/.local/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0) (4.53.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/elifohri/.local/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0) (1.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->stable-baselines3==2.0.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/elifohri/.local/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in /home/elifohri/.local/lib/python3.10/site-packages (from matplotlib->stable-baselines3==2.0.0) (10.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->stable-baselines3==2.0.0) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/elifohri/.local/lib/python3.10/site-packages (from pandas->stable-baselines3==2.0.0) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/elifohri/.local/lib/python3.10/site-packages (from sympy->torch>=1.11->stable-baselines3==2.0.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# First, install stable baselines; only SB3 v2.0.0+ supports Gymnasium\n",
    "%pip install stable-baselines3==2.0.0 tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 200,\n",
       " 'height': 200,\n",
       " 'EP_MAX_TIME': 100,\n",
       " 'seed': 666,\n",
       " 'reset_rng_episode': False,\n",
       " 'arrival': mobile_env.core.arrival.NoDeparture,\n",
       " 'channel': mobile_env.core.channels.OkumuraHata,\n",
       " 'scheduler': mobile_env.core.schedules.RoundRobin,\n",
       " 'movement': mobile_env.core.movement.RandomWaypointMovement,\n",
       " 'utility': mobile_env.core.utilities.BoundedLogUtility,\n",
       " 'handler': mobile_env.handlers.smart_city_handler.MComSmartCityHandler,\n",
       " 'bs': {'bw': 100000000.0,\n",
       "  'freq': 2500,\n",
       "  'tx': 40,\n",
       "  'height': 50,\n",
       "  'computational_power': 100},\n",
       " 'ue': {'velocity': 1.5, 'snr_tr': 2e-08, 'noise': 1e-09, 'height': 1.5},\n",
       " 'sensor': {'height': 1.5,\n",
       "  'snr_tr': 2e-08,\n",
       "  'noise': 1e-09,\n",
       "  'velocity': 0,\n",
       "  'radius': 500,\n",
       "  'logs': {}},\n",
       " 'ue_job': {'job_generation_probability': 0.7,\n",
       "  'communication_job_lambda_value': 2.875,\n",
       "  'computation_job_lambda_value': 10.0},\n",
       " 'sensor_job': {'communication_job_lambda_value': 1.125,\n",
       "  'computation_job_lambda_value': 5.0},\n",
       " 'e2e_delay_threshold': 5,\n",
       " 'reward_calculation': {'ue_penalty': -3,\n",
       "  'discount_factor': 0.9,\n",
       "  'base_reward': 10,\n",
       "  'positive_discount_factor': 0.9,\n",
       "  'negative_discount_factor': 0.8},\n",
       " 'arrival_params': {'ep_time': 100, 'reset_rng_episode': False},\n",
       " 'channel_params': {},\n",
       " 'scheduler_params': {'quantum': 2.0},\n",
       " 'movement_params': {'width': 200, 'height': 200, 'reset_rng_episode': False},\n",
       " 'utility_params': {'lower': -20, 'upper': 20, 'coeffs': (10, 0, 10)},\n",
       " 'metrics': {'scalar_metrics': {},\n",
       "  'ue_metrics': {},\n",
       "  'bs_metrics': {},\n",
       "  'ss_metrics': {}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import gymnasium\n",
    "import mobile_env\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# predefined small scenarios\n",
    "from mobile_env.scenarios.smart_city import MComSmartCity\n",
    "\n",
    "# easy access to the default configuration\n",
    "MComSmartCity.default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "\n",
    "# Register the new environment\n",
    "register(\n",
    "    id='mobile-smart_city-smart_city_handler-rl-v0',\n",
    "    entry_point='mobile_env.scenarios.smart_city:MComSmartCity',  # Adjust this if the entry point is different\n",
    "    kwargs={'config': {}, 'render_mode': None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'CartPoleJax-v0', 'CartPoleJax-v1', 'PendulumJax-v0', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Jax-Blackjack-v0', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'GymV21Environment-v0', 'GymV26Environment-v0', 'mobile-smart_city-smart_city_handler-v0', 'mobile-smart_city-smart_city_handler-rl-v0'])\n",
      "Environment 'mobile-smart_city-smart_city_handler-rl-v0' registered successfully!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# List all registered environments\n",
    "env_specs = gym.envs.registry.keys()\n",
    "print(env_specs)\n",
    "\n",
    "# Verify your specific environment is listed\n",
    "assert 'mobile-smart_city-smart_city_handler-rl-v0' in env_specs, \"Environment not registered correctly\"\n",
    "print(\"Environment 'mobile-smart_city-smart_city_handler-rl-v0' registered successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Smart city environment for RL with 5 users, 15 sensors and 1 cells.\n"
     ]
    }
   ],
   "source": [
    "# create a small mobile environment for a single, centralized control agent\n",
    "# pass rgb_array as render mode so the env can be rendered inside the notebook\n",
    "env = gymnasium.make(\"mobile-smart_city-smart_city_handler-rl-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(f\"\\nSmart city environment for RL with {env.NUM_USERS} users, {env.NUM_SENSORS} sensors and {env.NUM_STATIONS} cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to results_sb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -235     |\n",
      "| time/              |          |\n",
      "|    fps             | 8        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -132        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1           |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2885        |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004874114 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | -0.00164    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.4e+03     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 1.5e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -56.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3119        |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005886377 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.039       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.25e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00668    |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 1.52e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | 80.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 3356         |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062353765 |\n",
      "|    clip_fraction        | 0.0628       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.0656       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.77e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0088      |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 1.51e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | 216         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2           |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 3576        |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008723317 |\n",
      "|    clip_fraction        | 0.0987      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.0703      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.83e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00984    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 1.56e+04    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train a Single-Agent Reinforcement Learning\n",
    "\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Wrap the environment with Monitor\n",
    "env = Monitor(env)\n",
    "\n",
    "# Define the model for training (PPO algorithm)\n",
    "model = PPO(MlpPolicy, env, tensorboard_log='results_sb', verbose=1)\n",
    "\n",
    "# Create an evaluation environment for logging\n",
    "eval_env = gymnasium.make(\"mobile-smart_city-smart_city_handler-rl-v0\", render_mode=\"rgb_array\")\n",
    "eval_env = Monitor(eval_env)  # Wrap the eval environment with Monitor\n",
    "\n",
    "# Train the model for a set number of timesteps\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the trained model for future use\n",
    "model.save(\"ppo_mobile_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elifohri/.local/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:437: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  th_object = th.load(file_content, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Test the Trained Model\n",
    "\n",
    "# Load the saved model\n",
    "model = PPO.load(\"ppo_mobile_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Use the trained model to predict the action\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Take the action in the environment\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:555\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    537\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    541\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py:346\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_training_mode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 346\u001b[0m observation, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    349\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(observation, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py:260\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    257\u001b[0m     observation \u001b[38;5;241m=\u001b[39m maybe_transpose(observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     vectorized_env \u001b[38;5;241m=\u001b[39m is_vectorized_observation(observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Step 6: Test the model in the environment\n",
    "\n",
    "obs = env.reset()\n",
    "for step in range(200):\n",
    "    # Use the trained model to predict the action\n",
    "    action, _states = model.predict(obs)\n",
    "    \n",
    "    # Take the action in the environment\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Print observation and reward\n",
    "    print(f\"Step {step+1} | Action: {action} | Observation: {obs} | Reward: {reward}\")\n",
    "    \n",
    "    # Break the loop if the episode is finished\n",
    "    if done:\n",
    "        print(\"Episode finished\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Plot Results\n",
    "\n",
    "# Example of plotting some metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example plotting of dummy reward over episodes (assuming we have a list of rewards)\n",
    "# This is just an illustrative example - you'll need to replace this with your own logic for recording rewards\n",
    "rewards = [np.random.uniform(-1, 1) for _ in range(100)]  # Replace with actual data\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Reward Over Episodes\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
