{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating `mobile-env:smart-city`\n",
    "\n",
    "`mobile-env` is a simple and open environment for training, testing, and evaluating a decentralized metaverse environment.\n",
    "\n",
    "* `mobile-env:smart-city` is written in pure Python\n",
    "* It allows simulating various scenarios with moving users in a cellular network with a single base station and multiple stationary sensors\n",
    "* `mobile-env:smart-city` implements the standard [Gymnasium](https://gymnasium.farama.org/) (previously [OpenAI Gym](https://gym.openai.com/)) interface such that it can be used with all common frameworks for reinforcement learning\n",
    "* `mobile-env:smart-city` is not restricted to reinforcement learning approaches but can also be used with conventional control approaches or dummy benchmark algorithms\n",
    "* It can be configured easily (e.g., adjusting number and movement of users, properties of cells, etc.)\n",
    "* It is also easy to extend `mobile-env:smart-city`, e.g., implementing different observations, actions, or reward\n",
    "\n",
    "As such `mobile-env:smart-city` is a simple platform to test RL algorithms in a decentralized metaverse environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration Steps:**\n",
    "\n",
    "This demonstration consists of the following steps:\n",
    "\n",
    "1. Installation and usage of `mobile-env` with dummy actions\n",
    "2. Configuration of `mobile-env` and adjustment of the observation space (optional)\n",
    "3. Training a single-agent reinforcement learning approach with [`stable-baselines3`](https://github.com/DLR-RM/stable-baselines3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3==2.0.0 in /home/elifori/.local/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: tensorboard in /home/elifori/.local/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: gymnasium==0.28.1 in /home/elifori/.local/lib/python3.9/site-packages (from stable-baselines3==2.0.0) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/elifori/.local/lib/python3.9/site-packages (from stable-baselines3==2.0.0) (2.0.2)\n",
      "Requirement already satisfied: torch>=1.11 in /home/elifori/.local/lib/python3.9/site-packages (from stable-baselines3==2.0.0) (2.5.1)\n",
      "Requirement already satisfied: pandas in /home/elifori/.local/lib/python3.9/site-packages (from stable-baselines3==2.0.0) (2.2.3)\n",
      "Requirement already satisfied: cloudpickle in /home/elifori/.local/lib/python3.9/site-packages (from stable-baselines3==2.0.0) (3.1.0)\n",
      "Requirement already satisfied: matplotlib in /home/elifori/.local/lib/python3.9/site-packages (from stable-baselines3==2.0.0) (3.9.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/elifori/.local/lib/python3.9/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/elifori/.local/lib/python3.9/site-packages (from tensorboard) (5.28.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard) (45.2.0)\n",
      "Requirement already satisfied: packaging in /home/elifori/.local/lib/python3.9/site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/elifori/.local/lib/python3.9/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/elifori/.local/lib/python3.9/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/elifori/.local/lib/python3.9/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/elifori/.local/lib/python3.9/site-packages (from tensorboard) (1.68.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/elifori/.local/lib/python3.9/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (4.12.2)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /home/elifori/.local/lib/python3.9/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0; python_version < \"3.10\" in /home/elifori/.local/lib/python3.9/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (8.5.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/elifori/.local/lib/python3.9/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (0.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (10.3.5.147)\n",
      "Requirement already satisfied: filelock in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.16.1)\n",
      "Requirement already satisfied: triton==3.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1; python_version >= \"3.9\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (12.4.5.8)\n",
      "Requirement already satisfied: fsspec in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (9.1.0.70)\n",
      "Requirement already satisfied: jinja2 in /home/elifori/.local/lib/python3.9/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.1.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/elifori/.local/lib/python3.9/site-packages (from pandas->stable-baselines3==2.0.0) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/elifori/.local/lib/python3.9/site-packages (from pandas->stable-baselines3==2.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/elifori/.local/lib/python3.9/site-packages (from pandas->stable-baselines3==2.0.0) (2024.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/elifori/.local/lib/python3.9/site-packages (from matplotlib->stable-baselines3==2.0.0) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/elifori/.local/lib/python3.9/site-packages (from matplotlib->stable-baselines3==2.0.0) (3.2.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/elifori/.local/lib/python3.9/site-packages (from matplotlib->stable-baselines3==2.0.0) (6.4.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/elifori/.local/lib/python3.9/site-packages (from matplotlib->stable-baselines3==2.0.0) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/elifori/.local/lib/python3.9/site-packages (from matplotlib->stable-baselines3==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/elifori/.local/lib/python3.9/site-packages (from matplotlib->stable-baselines3==2.0.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/elifori/.local/lib/python3.9/site-packages (from matplotlib->stable-baselines3==2.0.0) (4.55.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/elifori/.local/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/elifori/.local/lib/python3.9/site-packages (from importlib-metadata>=4.8.0; python_version < \"3.10\"->gymnasium==0.28.1->stable-baselines3==2.0.0) (3.21.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/elifori/.local/lib/python3.9/site-packages (from sympy==1.13.1; python_version >= \"3.9\"->torch>=1.11->stable-baselines3==2.0.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# First, install stable baselines; only SB3 v2.0.0+ supports Gymnasium\n",
    "%pip install stable-baselines3==2.0.0 tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 23:21:47.209066: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731795707.228413    6931 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731795707.233650    6931 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-16 23:21:47.255860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'width': 200,\n",
       " 'height': 200,\n",
       " 'EP_MAX_TIME': 100,\n",
       " 'seed': 666,\n",
       " 'reset_rng_episode': False,\n",
       " 'arrival': mobile_env.core.arrival.NoDeparture,\n",
       " 'channel': mobile_env.core.channels.OkumuraHata,\n",
       " 'scheduler': mobile_env.core.schedules.RoundRobin,\n",
       " 'movement': mobile_env.core.movement.RandomWaypointMovement,\n",
       " 'utility': mobile_env.core.utilities.BoundedLogUtility,\n",
       " 'handler': mobile_env.handlers.smart_city_handler.MComSmartCityHandler,\n",
       " 'bs': {'bw': 100000000.0,\n",
       "  'freq': 2500,\n",
       "  'tx': 40,\n",
       "  'height': 50,\n",
       "  'computational_power': 100},\n",
       " 'ue': {'velocity': 1.5, 'snr_tr': 2e-08, 'noise': 1e-09, 'height': 1.5},\n",
       " 'sensor': {'height': 1.5,\n",
       "  'snr_tr': 2e-08,\n",
       "  'noise': 1e-09,\n",
       "  'velocity': 0,\n",
       "  'radius': 500,\n",
       "  'logs': {}},\n",
       " 'ue_job': {'job_generation_probability': 0.7,\n",
       "  'communication_job_lambda_value': 2.875,\n",
       "  'computation_job_lambda_value': 10.0},\n",
       " 'sensor_job': {'communication_job_lambda_value': 1.125,\n",
       "  'computation_job_lambda_value': 5.0},\n",
       " 'e2e_delay_threshold': 5,\n",
       " 'reward_calculation': {'ue_penalty': -3,\n",
       "  'discount_factor': 0.9,\n",
       "  'base_reward': 10,\n",
       "  'positive_discount_factor': 0.9,\n",
       "  'negative_discount_factor': 0.8},\n",
       " 'arrival_params': {'ep_time': 100, 'reset_rng_episode': False},\n",
       " 'channel_params': {},\n",
       " 'scheduler_params': {'quantum': 2.0},\n",
       " 'movement_params': {'width': 200, 'height': 200, 'reset_rng_episode': False},\n",
       " 'utility_params': {'lower': -20, 'upper': 20, 'coeffs': (10, 0, 10)},\n",
       " 'metrics': {'scalar_metrics': {},\n",
       "  'ue_metrics': {},\n",
       "  'bs_metrics': {},\n",
       "  'ss_metrics': {}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import gymnasium\n",
    "import mobile_env\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# predefined small scenarios\n",
    "from mobile_env.scenarios.smart_city import MComSmartCity\n",
    "\n",
    "# easy access to the default configuration\n",
    "MComSmartCity.default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "\n",
    "# Register the new environment\n",
    "register(\n",
    "    id='mobile-smart_city-smart_city_handler-rl-v0',\n",
    "    entry_point='mobile_env.scenarios.smart_city:MComSmartCity',  # Adjust this if the entry point is different\n",
    "    kwargs={'config': {}, 'render_mode': None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'CartPoleJax-v0', 'CartPoleJax-v1', 'PendulumJax-v0', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Jax-Blackjack-v0', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'GymV21Environment-v0', 'GymV26Environment-v0', 'mobile-smart_city-smart_city_handler-v0', 'mobile-smart_city-smart_city_handler-rl-v0'])\n",
      "Environment 'mobile-smart_city-smart_city_handler-rl-v0' registered successfully!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# List all registered environments\n",
    "env_specs = gym.envs.registry.keys()\n",
    "print(env_specs)\n",
    "\n",
    "# Verify your specific environment is listed\n",
    "assert 'mobile-smart_city-smart_city_handler-rl-v0' in env_specs, \"Environment not registered correctly\"\n",
    "print(\"Environment 'mobile-smart_city-smart_city_handler-rl-v0' registered successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Smart city environment for RL with 10 users, 20 sensors and 1 cells.\n"
     ]
    }
   ],
   "source": [
    "# create a small mobile environment for a single, centralized control agent\n",
    "# pass rgb_array as render mode so the env can be rendered inside the notebook\n",
    "env = gymnasium.make(\"mobile-smart_city-smart_city_handler-rl-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(f\"\\nSmart city environment for RL with {env.NUM_USERS} users, {env.NUM_SENSORS} sensors and {env.NUM_STATIONS} cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train a Single-Agent Reinforcement Learning\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# Custom TensorBoard Callback\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=1):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        # Log custom metrics to TensorBoard\n",
    "        reward = self.locals['rewards'][-1] if 'rewards' in self.locals else 0\n",
    "        self.logger.record('custom/reward', reward)\n",
    "        return True\n",
    "\n",
    "# Wrapping the environment with a TimeLimit wrapper to enforce 200 timesteps per episode\n",
    "def wrap_environment(env_name, max_episode_steps=200):\n",
    "    raw_env = gymnasium.make(env_name)\n",
    "    return TimeLimit(raw_env, max_episode_steps=max_episode_steps)\n",
    "\n",
    "# Train RL Model\n",
    "def train_rl_model(env_name, eval_env_name):\n",
    "    \"\"\"Train a PPO RL model with callbacks and logging.\"\"\"\n",
    "    # Wrap environments with TimeLimit\n",
    "    env = wrap_environment(env_name, max_episode_steps=100)\n",
    "    eval_env = wrap_environment(eval_env_name, max_episode_steps=100)\n",
    "\n",
    "    # Wrap environments with Monitor for logging\n",
    "    env = Monitor(env)\n",
    "    eval_env = Monitor(eval_env)\n",
    "\n",
    "    # Logger setup\n",
    "    log_dir = \"results_sb\"\n",
    "    new_logger = configure(log_dir, [\"tensorboard\"])\n",
    "    \n",
    "    # Define model\n",
    "    model = PPO(\"MlpPolicy\", env, tensorboard_log=log_dir, verbose=1)\n",
    "    model.set_logger(new_logger)\n",
    "    \n",
    "    # Define callbacks\n",
    "    eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/best_model',\n",
    "                                 log_path='./logs/results', eval_freq=500)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/checkpoints/',\n",
    "                                             name_prefix='ppo_smartcity')\n",
    "    tensorboard_callback = TensorboardCallback()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    model.learn(total_timesteps=10000, callback=[eval_callback, checkpoint_callback, tensorboard_callback])\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"ppo_smartcity_model\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# To visualize the logs, run `tensorboard --logdir results_sb` in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting training...\n",
      "Eval num_timesteps=500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2500, episode_reward=7.40 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=3.60 +/- 4.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=11.02 +/- 8.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=9.20 +/- 5.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6500, episode_reward=-160.92 +/- 70.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-123.22 +/- 74.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7500, episode_reward=-134.72 +/- 23.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-162.24 +/- 48.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8500, episode_reward=137.07 +/- 59.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=69.48 +/- 123.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9500, episode_reward=115.30 +/- 46.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=82.45 +/- 56.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trained_model = train_rl_model(\"mobile-smart_city-smart_city_handler-rl-v0\", \"mobile-smart_city-smart_city_handler-rl-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Test the Trained Model\n",
    "\n",
    "# Load the saved model\n",
    "model = PPO.load(\"ppo_mobile_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_lengths = np.array(env.get_queue_lengths()).ravel()\n",
    "print(f\"Queue lengths shape: {queue_lengths.shape}, values: {queue_lengths}\")\n",
    "\n",
    "resource_utilization = np.array(env.get_resource_utilization()).ravel()\n",
    "print(f\"Resource utilization shape: {resource_utilization.shape}, values: {resource_utilization}\")\n",
    "\n",
    "print(f\"Environment observation space: {env.observation_space}\")\n",
    "print(f\"Model observation space: {model.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Test the model in the environment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "done = False\n",
    "obs, info = env.reset()\n",
    "\n",
    "total_episode_reward = 0\n",
    "total_reward_over_time = []  # List to store the reward at each time step\n",
    "\n",
    "for step in range(100):\n",
    "    # Extract the array part of the observation, ignoring the empty dictionary\n",
    "\n",
    "    # Use the trained model to predict the action\n",
    "    action, _states = model.predict(obs)\n",
    "\n",
    "    # Take the action in the environment\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    total_episode_reward += reward\n",
    "    total_reward_over_time.append(total_episode_reward) \n",
    "\n",
    "    # Print observation and reward\n",
    "    print(f\"Step {step+1} | Action: {action} | Observation: {obs[0]} | Reward: {reward}\")\n",
    "    \n",
    "    # render the environment\n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the reward over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(total_reward_over_time) + 1), total_reward_over_time, marker='o')\n",
    "plt.title('Total Reward Over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Plot Results\n",
    "\n",
    "# Example of plotting some metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example plotting of dummy reward over episodes (assuming we have a list of rewards)\n",
    "# This is just an illustrative example - you'll need to replace this with your own logic for recording rewards\n",
    "rewards = [np.random.uniform(-1, 1) for _ in range(100)]  # Replace with actual data\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Reward Over Episodes\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
