{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating `mobile-env:smart-city`\n",
    "\n",
    "`mobile-env` is a simple and open environment for training, testing, and evaluating a decentralized metaverse environment.\n",
    "\n",
    "* `mobile-env:smart-city` is written in pure Python\n",
    "* It allows simulating various scenarios with moving users in a cellular network with a single base station and multiple stationary sensors\n",
    "* `mobile-env:smart-city` implements the standard [Gymnasium](https://gymnasium.farama.org/) (previously [OpenAI Gym](https://gym.openai.com/)) interface such that it can be used with all common frameworks for reinforcement learning\n",
    "* `mobile-env:smart-city` is not restricted to reinforcement learning approaches but can also be used with conventional control approaches or dummy benchmark algorithms\n",
    "* It can be configured easily (e.g., adjusting number and movement of users, properties of cells, etc.)\n",
    "* It is also easy to extend `mobile-env:smart-city`, e.g., implementing different observations, actions, or reward\n",
    "\n",
    "As such `mobile-env:smart-city` is a simple platform to test RL algorithms in a decentralized metaverse environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration Steps:**\n",
    "\n",
    "This demonstration consists of the following steps:\n",
    "\n",
    "1. Installation and usage of `mobile-env` with dummy actions\n",
    "2. Configuration of `mobile-env` and adjustment of the observation space (optional)\n",
    "3. Training a single-agent reinforcement learning approach with [`stable-baselines3`](https://github.com/DLR-RM/stable-baselines3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stable-baselines3==2.0.0 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (2.0.0)\n",
      "Requirement already satisfied: tensorboard in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from stable-baselines3==2.0.0) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from stable-baselines3==2.0.0) (3.1.0)\n",
      "Requirement already satisfied: gymnasium==0.28.1 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from stable-baselines3==2.0.0) (0.28.1)\n",
      "Requirement already satisfied: pandas in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from stable-baselines3==2.0.0) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from stable-baselines3==2.0.0) (3.9.3)\n",
      "Requirement already satisfied: torch>=1.11 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from stable-baselines3==2.0.0) (2.5.1)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0) (0.0.4)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from tensorboard) (1.68.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: six>1.9 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard) (1.15.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from tensorboard) (5.29.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard) (58.0.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: packaging in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.8.0->gymnasium==0.28.1->stable-baselines3==2.0.0) (3.21.0)\n",
      "Requirement already satisfied: filelock in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.1.4)\n",
      "Requirement already satisfied: networkx in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from torch>=1.11->stable-baselines3==2.0.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.11->stable-baselines3==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from matplotlib->stable-baselines3==2.0.0) (4.55.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from matplotlib->stable-baselines3==2.0.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from matplotlib->stable-baselines3==2.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from matplotlib->stable-baselines3==2.0.0) (6.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from matplotlib->stable-baselines3==2.0.0) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from matplotlib->stable-baselines3==2.0.0) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from matplotlib->stable-baselines3==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from matplotlib->stable-baselines3==2.0.0) (11.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from pandas->stable-baselines3==2.0.0) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/elifohri/Library/Python/3.9/lib/python/site-packages (from pandas->stable-baselines3==2.0.0) (2024.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# First, install stable baselines; only SB3 v2.0.0+ supports Gymnasium\n",
    "%pip install stable-baselines3==2.0.0 tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 200,\n",
       " 'height': 200,\n",
       " 'EP_MAX_TIME': 100,\n",
       " 'seed': 666,\n",
       " 'reset_rng_episode': False,\n",
       " 'arrival': mobile_env.core.arrival.NoDeparture,\n",
       " 'channel': mobile_env.core.channels.OkumuraHata,\n",
       " 'scheduler': mobile_env.core.schedules.ResourceFair,\n",
       " 'movement': mobile_env.core.movement.RandomWaypointMovement,\n",
       " 'utility': mobile_env.core.utilities.BoundedLogUtility,\n",
       " 'handler': mobile_env.handlers.smart_city_handler.MComSmartCityHandler,\n",
       " 'bs': {'bw': 100000000.0,\n",
       "  'freq': 2500,\n",
       "  'tx': 40,\n",
       "  'height': 50,\n",
       "  'computational_power': 100},\n",
       " 'ue': {'velocity': 1.5, 'snr_tr': 2e-08, 'noise': 1e-09, 'height': 1.5},\n",
       " 'sensor': {'height': 1.5, 'snr_tr': 2e-08, 'noise': 1e-09},\n",
       " 'ue_job': {'job_generation_probability': 0.7,\n",
       "  'communication_job_lambda_value': 10.0,\n",
       "  'computation_job_lambda_value': 10.0},\n",
       " 'sensor_job': {'communication_job_lambda_value': 5.0,\n",
       "  'computation_job_lambda_value': 5.0},\n",
       " 'e2e_delay_threshold': 3.0,\n",
       " 'reward_calculation': {'ue_penalty': -5.0,\n",
       "  'discount_factor': 0.95,\n",
       "  'base_reward': 10.0,\n",
       "  'positive_discount_factor': 0.9,\n",
       "  'negative_discount_factor': 0.8},\n",
       " 'arrival_params': {'ep_time': 100, 'reset_rng_episode': False},\n",
       " 'channel_params': {},\n",
       " 'scheduler_params': {'quantum': 2.0},\n",
       " 'movement_params': {'width': 200, 'height': 200, 'reset_rng_episode': False},\n",
       " 'utility_params': {'lower': -20, 'upper': 20, 'coeffs': (10, 0, 10)},\n",
       " 'metrics': {'scalar_metrics': {},\n",
       "  'kpi_metrics': {},\n",
       "  'ue_metrics': {},\n",
       "  'bs_metrics': {},\n",
       "  'ss_metrics': {}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import gymnasium\n",
    "import mobile_env\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# predefined small scenarios\n",
    "from mobile_env.scenarios.smart_city import MComSmartCity\n",
    "\n",
    "# easy access to the default configuration\n",
    "MComSmartCity.default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "\n",
    "# Register the new environment\n",
    "register(\n",
    "    id='mobile-smart_city-smart_city_handler-rl-v0',\n",
    "    entry_point='mobile_env.scenarios.smart_city:MComSmartCity',  # Adjust this if the entry point is different\n",
    "    kwargs={'config': {}, 'render_mode': None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train a Single-Agent Reinforcement Learning\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# Custom TensorBoard Callback\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=1):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        # Log reward\n",
    "        reward = self.locals['rewards'][-1] if 'rewards' in self.locals else 0\n",
    "        self.logger.record('custom/reward', reward)\n",
    "        \n",
    "        # Log mean action\n",
    "        actions = self.locals['actions'] if 'actions' in self.locals else np.array([])\n",
    "        mean_action = np.mean(actions) if actions.size > 0 else 0\n",
    "        self.logger.record('custom/mean_action', mean_action)\n",
    "        \n",
    "        # Log policy loss (optional)\n",
    "        policy_loss = self.locals['policy_loss'] if 'policy_loss' in self.locals else 0\n",
    "        self.logger.record('custom/policy_loss', policy_loss)\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Wrapping the environment with a TimeLimit wrapper to enforce 200 timesteps per episode\n",
    "def wrap_environment(env_name, max_episode_steps=100):\n",
    "    env = gymnasium.make(env_name)\n",
    "    env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "# Train RL Model\n",
    "def train_rl_model(env_name, eval_env_name):\n",
    "    \"\"\"Train a PPO RL model with callbacks and logging.\"\"\"\n",
    "    # Wrap training and evaluation environments\n",
    "    env = wrap_environment(env_name, max_episode_steps=100)\n",
    "    eval_env = wrap_environment(eval_env_name, max_episode_steps=100)\n",
    "\n",
    "    # Logger setup\n",
    "    log_dir = \"results_sb\"\n",
    "    new_logger = configure(log_dir, [\"tensorboard\"])\n",
    "    \n",
    "    # Define model\n",
    "    model = PPO(\"MlpPolicy\", env, tensorboard_log=log_dir, verbose=1)\n",
    "    model.set_logger(new_logger)\n",
    "    \n",
    "    # Define callbacks\n",
    "    eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/best_model',\n",
    "                                 log_path='./logs/results', eval_freq=500)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/checkpoints/',\n",
    "                                             name_prefix='ppo_smartcity')\n",
    "    tensorboard_callback = TensorboardCallback()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    model.learn(total_timesteps=3000, callback=[eval_callback, checkpoint_callback, tensorboard_callback])\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"ppo_smartcity_model\")\n",
    "    return model\n",
    "\n",
    "# To visualize the logs, run `tensorboard --logdir results_sb` in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting training...\n",
      "Eval num_timesteps=500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trained_model = train_rl_model(\"mobile-smart_city-smart_city_handler-rl-v0\", \"mobile-smart_city-smart_city_handler-rl-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to results_sb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -627     |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 100            |\n",
      "|    ep_rew_mean          | -592           |\n",
      "| time/                   |                |\n",
      "|    fps                  | 102            |\n",
      "|    iterations           | 2              |\n",
      "|    time_elapsed         | 40             |\n",
      "|    total_timesteps      | 4096           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.001124599    |\n",
      "|    clip_fraction        | 9.77e-05       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -2.83          |\n",
      "|    explained_variance   | -7.6293945e-06 |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 6.73e+03       |\n",
      "|    n_updates            | 10             |\n",
      "|    policy_gradient_loss | -0.00103       |\n",
      "|    std                  | 0.998          |\n",
      "|    value_loss           | 1.51e+04       |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -598         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 102          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.001420215  |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.0053067207 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.2e+03      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.34e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -589         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 101          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012107771 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.003868997  |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.25e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 1.37e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -596         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 101          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 101          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011339153 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.002527833  |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.97e+03     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 1.25e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -606         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 101          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 121          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032980847 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.0010317564 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.59e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00329     |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 1.35e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 100           |\n",
      "|    ep_rew_mean          | -609          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 101           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 140           |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.006341819   |\n",
      "|    clip_fraction        | 0.0339        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.79         |\n",
      "|    explained_variance   | 0.00058192015 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.21e+03      |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.00583      |\n",
      "|    std                  | 0.969         |\n",
      "|    value_loss           | 1.24e+04      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -571         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 102          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 160          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004062728  |\n",
      "|    clip_fraction        | 0.04         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.0004721284 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.78e+03     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00812     |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 9.99e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -549         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 102          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 179          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044693947 |\n",
      "|    clip_fraction        | 0.0427       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.0007248521 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.45e+03     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00764     |\n",
      "|    std                  | 0.95         |\n",
      "|    value_loss           | 6.85e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -498         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 103          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 198          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055189095 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 1.347065e-05 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77e+03     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00811     |\n",
      "|    std                  | 0.927        |\n",
      "|    value_loss           | 5.68e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 100           |\n",
      "|    ep_rew_mean          | -431          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 103           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 217           |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.005761444   |\n",
      "|    clip_fraction        | 0.0485        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.65         |\n",
      "|    explained_variance   | 2.0861626e-06 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.82e+03      |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.0077       |\n",
      "|    std                  | 0.904         |\n",
      "|    value_loss           | 4.81e+03      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 100            |\n",
      "|    ep_rew_mean          | -373           |\n",
      "| time/                   |                |\n",
      "|    fps                  | 104            |\n",
      "|    iterations           | 12             |\n",
      "|    time_elapsed         | 235            |\n",
      "|    total_timesteps      | 24576          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0059785936   |\n",
      "|    clip_fraction        | 0.0574         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -2.62          |\n",
      "|    explained_variance   | -2.5749207e-05 |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 1.55e+03       |\n",
      "|    n_updates            | 110            |\n",
      "|    policy_gradient_loss | -0.00859       |\n",
      "|    std                  | 0.899          |\n",
      "|    value_loss           | 2.6e+03        |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 100           |\n",
      "|    ep_rew_mean          | -331          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 104           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 254           |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00610926    |\n",
      "|    clip_fraction        | 0.0501        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.58         |\n",
      "|    explained_variance   | -1.168251e-05 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.19e+03      |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.00824      |\n",
      "|    std                  | 0.875         |\n",
      "|    value_loss           | 2.15e+03      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 100            |\n",
      "|    ep_rew_mean          | -280           |\n",
      "| time/                   |                |\n",
      "|    fps                  | 105            |\n",
      "|    iterations           | 14             |\n",
      "|    time_elapsed         | 272            |\n",
      "|    total_timesteps      | 28672          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.006514588    |\n",
      "|    clip_fraction        | 0.0511         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -2.52          |\n",
      "|    explained_variance   | -0.00018656254 |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 840            |\n",
      "|    n_updates            | 130            |\n",
      "|    policy_gradient_loss | -0.00721       |\n",
      "|    std                  | 0.851          |\n",
      "|    value_loss           | 1.61e+03       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 100           |\n",
      "|    ep_rew_mean          | -229          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 105           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 290           |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.018251296   |\n",
      "|    clip_fraction        | 0.0969        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.46         |\n",
      "|    explained_variance   | -5.340576e-05 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 570           |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.00688      |\n",
      "|    std                  | 0.831         |\n",
      "|    value_loss           | 1.12e+03      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x14c286070>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"mobile-smart_city-smart_city_handler-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "# train PPO agent on environment. this takes a while\n",
    "model = PPO(MlpPolicy, env, tensorboard_log='results_sb', verbose=1)\n",
    "model.learn(total_timesteps=30000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
